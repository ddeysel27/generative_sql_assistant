{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875e0af7",
   "metadata": {},
   "source": [
    "## Import model from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4060c367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SentencePiece loaded, version: 0.2.1\n",
      "âœ… PyTorch loaded, version: 2.8.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "print(\"âœ… SentencePiece loaded, version:\", spm.__version__)\n",
    "\n",
    "import torch\n",
    "print(\"âœ… PyTorch loaded, version:\", torch.__version__) \n",
    "print(\"CUDA available:\", torch.cuda.is_available()) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "060d2d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9763b321dcc4c5e8401ac838cfcf304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Data Science\\Personal Projects\\Text to SQL Assistant\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd992f0d9aa447c981231a063586f1af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6471f5510cd343058b54ed64a5279330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac62507516b4490ebf7c1fde687fdeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\Data Science\\Personal Projects\\Text to SQL Assistant\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--cssupport--t5-small-awesome-text-to-sql. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5c5411a791449c8e1daced0ee78bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated SQL query is: SELECT student_id FROM students WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319e7758fe5d435b82c8be406169b2ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# Initialize the tokenizer from Hugging Face Transformers library\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "\n",
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = T5ForConditionalGeneration.from_pretrained('cssupport/t5-small-awesome-text-to-sql')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_sql(input_prompt):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(input_prompt, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_length=512)\n",
    "    \n",
    "    # Decode the output IDs to a string (SQL query in this case)\n",
    "    generated_sql = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_sql\n",
    "\n",
    "\n",
    "input_prompt = \"tables:\\n\" + \"CREATE TABLE student_course_attendance (student_id VARCHAR); CREATE TABLE students (student_id VARCHAR)\" + \"\\n\" + \"query for:\" + \"List the id of students who never attends courses?\"\n",
    "\n",
    "generated_sql = generate_sql(input_prompt)\n",
    "\n",
    "print(f\"The generated SQL query is: {generated_sql}\")\n",
    "#OUTPUT: The generated SQL query is: SELECT student_id FROM students WHERE NOT student_id IN (SELECT student_id FROM student_course_attendance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE Catalogs (date_of_latest_revision VARCHAR)\" + \"\\n\" +\"query for: Find the dates on which more than one revisions were made.\"\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE table_22767 ( \\\"Year\\\" real, \\\"World\\\" real, \\\"Asia\\\" text, \\\"Africa\\\" text, \\\"Europe\\\" text, \\\"Latin America/Caribbean\\\" text, \\\"Northern America\\\" text, \\\"Oceania\\\" text )\" + \"\\n\" +\"query for:what will the population of Asia be when Latin America/Caribbean is 783 (7.5%)?.\"\n",
    "#input_prompt = \"tables:\\n\" + \"CREATE TABLE procedures ( subject_id text, hadm_id text, icd9_code text, short_title text, long_title text ) CREATE TABLE diagnoses ( subject_id text, hadm_id text, icd9_code text, short_title text, long_title text ) CREATE TABLE lab ( subject_id text, hadm_id text, itemid text, charttime text, flag text, value_unit text, label text, fluid text ) CREATE TABLE demographic ( subject_id text, hadm_id text, name text, marital_status text, age text, dob text, gender text, language text, religion text, admission_type text, days_stay text, insurance text, ethnicity text, expire_flag text, admission_location text, discharge_location text, diagnosis text, dod text, dob_year text, dod_year text, admittime text, dischtime text, admityear text ) CREATE TABLE prescriptions ( subject_id text, hadm_id text, icustay_id text, drug_type text, drug text, formulary_drug_cd text, route text, drug_dose text )\" + \"\\n\" +\"query for:\" + \"what is the total number of patients who were diagnosed with icd9 code 2254?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aac1469c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT AVG(midd_house_value) FROM housing WHERE median_income > 5\n"
     ]
    }
   ],
   "source": [
    "input_prompt = \"\"\"\n",
    "tables:\n",
    "CREATE TABLE housing (\n",
    "    longitude REAL,\n",
    "    latitude REAL,\n",
    "    housing_median_age REAL,\n",
    "    total_rooms REAL,\n",
    "    total_bedrooms REAL,\n",
    "    population REAL,\n",
    "    households REAL,\n",
    "    median_income REAL,\n",
    "    median_house_value REAL\n",
    ")\n",
    "query for: What is the average median_house_value for houses with median_income > 5?\n",
    "\"\"\"\n",
    "\n",
    "generated_sql = generate_sql(input_prompt)\n",
    "print(generated_sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bca10d",
   "metadata": {},
   "source": [
    "### Extract schema directly from SQLite database and feed it into the model prompt automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "14f7e48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, difflib, sqlite3, pandas as pd\n",
    "\n",
    "SQL_KEYWORDS = {\n",
    "    \"select\",\"from\",\"where\",\"and\",\"or\",\"not\",\"in\",\"between\",\"like\",\"is\",\"null\",\n",
    "    \"group\",\"by\",\"order\",\"limit\",\"offset\",\"asc\",\"desc\",\"avg\",\"sum\",\"min\",\"max\",\n",
    "    \"count\",\"distinct\",\"as\",\"on\",\"join\",\"left\",\"right\",\"inner\",\"outer\",\"having\"\n",
    "}\n",
    "SQL_FUNCS = {\"avg\",\"sum\",\"min\",\"max\",\"count\",\"abs\",\"round\",\"upper\",\"lower\",\"coalesce\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "7218ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3, pandas as pd, difflib, re, textwrap\n",
    "\n",
    "db_path = '../data/housing.db'\n",
    "# 1) read schema from DB\n",
    "def get_schema_and_columns(db_path: str, table_name: str):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(f\"PRAGMA table_info({table_name});\")\n",
    "    info = cur.fetchall()\n",
    "    conn.close()\n",
    "    if not info:\n",
    "        raise ValueError(f\"Table '{table_name}' not found in {db_path}\")\n",
    "    ddl = \"CREATE TABLE {t} (\\n{cols}\\n)\".format(\n",
    "        t=table_name,\n",
    "        cols=\",\\n\".join([f\"    {row[1]} {row[2]}\" for row in info])\n",
    "    )\n",
    "    columns = [row[1] for row in info]\n",
    "    return ddl, columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "38e37823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) normalize user synonyms before prompting \n",
    "ALIASES = {\n",
    "    # map synonyms (left) to real column names (right) in your DB\n",
    "    \"median_income\": \"MedInc\",\n",
    "    \"median house value\": \"MedHouseVal\",\n",
    "    \"house age\": \"HouseAge\",\n",
    "    \"avg rooms\": \"AveRooms\",\n",
    "    \"average rooms\": \"AveRooms\",\n",
    "    \"avg bedrooms\": \"AveBedrms\",\n",
    "    \"average bedrooms\": \"AveBedrms\",\n",
    "    \"lat\": \"Latitude\",\n",
    "    \"lng\": \"Longitude\",\n",
    "}\n",
    "\n",
    "def normalize_question(question: str) -> str:\n",
    "    for alias, column in ALIASES.items():\n",
    "        question = question.replace(alias, column)\n",
    "    return question\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8dd772dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3) prompt builder that constrains the model ---\n",
    "def build_prompt(db_path: str, table: str, question: str) -> str:\n",
    "    ddl, cols = get_schema_and_columns(db_path, table)\n",
    "    col_list = \", \".join(cols)\n",
    "    question = normalize_question(question)\n",
    "    rules = textwrap.dedent(f\"\"\"\n",
    "    Rules:\n",
    "    - Use ONLY these columns: {col_list}\n",
    "    - The table name is exactly `{table}` (no aliases).\n",
    "    - Return a SINGLE SQL statement, no commentary.\n",
    "    - Use reasonable numeric ranges (Latitude in [-90, 90], Longitude in [-180, 180]).\n",
    "    \"\"\").strip()\n",
    "    return f\"tables:\\n{ddl}\\n{rules}\\nquery for: {question}\", cols\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "93333b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: housing.db schema\n",
    "# db_path = '../data/housing.db'\n",
    "# table_name = 'housing'\n",
    "# schema = get_schema(db_path, table_name)\n",
    "# # \n",
    "# print(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6d1b8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plug-in: strict aliases to override hallucinations before fuzzy match ---\n",
    "STRICT_ALIASES = {\n",
    "    # hard typos seen in practice\n",
    "    \"HorseAge\": \"HouseAge\",\n",
    "    \"HypertAge\": \"HouseAge\",\n",
    "    \"House_Age\": \"HouseAge\",\n",
    "    \"Med_Inc\": \"MedInc\",\n",
    "    \"MedianIncome\": \"MedInc\",\n",
    "    \"Median_House_Value\": \"MedHouseVal\",\n",
    "    \"Lat\": \"Latitude\",\n",
    "    \"Long\": \"Longitude\",\n",
    "}\n",
    "\n",
    "def apply_strict_aliases(sql: str):\n",
    "    fixed = sql\n",
    "    for bad, good in STRICT_ALIASES.items():\n",
    "        # replace case-insensitively on word boundaries\n",
    "        fixed = re.sub(rf\"\\b{re.escape(bad)}\\b\", good, fixed, flags=re.I)\n",
    "    return fixed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7462e8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, difflib, sqlite3, pandas as pd\n",
    "\n",
    "SQL_KEYWORDS = {\n",
    "    \"select\",\"from\",\"where\",\"and\",\"or\",\"not\",\"in\",\"between\",\"like\",\"is\",\"null\",\n",
    "    \"group\",\"by\",\"order\",\"limit\",\"offset\",\"asc\",\"desc\",\"avg\",\"sum\",\"min\",\"max\",\n",
    "    \"count\",\"distinct\",\"as\",\"on\",\"join\",\"left\",\"right\",\"inner\",\"outer\",\"having\"\n",
    "}\n",
    "SQL_FUNCS = {\"avg\",\"sum\",\"min\",\"max\",\"count\",\"abs\",\"round\",\"upper\",\"lower\",\"coalesce\"}\n",
    "\n",
    "def tokenize_words(sql: str):\n",
    "    # words made of letters/underscore/digits; ignore quoted identifiers for simplicity\n",
    "    return re.findall(r\"[A-Za-z_][A-Za-z_0-9]*\", sql)\n",
    "\n",
    "def enforce_columns(sql: str, cols: list, table_name: str):\n",
    "    \"\"\"Replace any unknown identifiers with closest valid column before running EXPLAIN.\"\"\"\n",
    "    col_set = {c.lower(): c for c in cols}\n",
    "    table_lc = table_name.lower()\n",
    "\n",
    "    tokens = tokenize_words(sql)\n",
    "    replacements = {}\n",
    "    for tok in tokens:\n",
    "        tl = tok.lower()\n",
    "        if (\n",
    "            tl in SQL_KEYWORDS\n",
    "            or tl in SQL_FUNCS\n",
    "            or tl == table_lc\n",
    "            or tl.isdigit()\n",
    "        ):\n",
    "            continue\n",
    "        if tl not in col_set:  # unknown identifier -> try to map\n",
    "            # fuzzy match against columns\n",
    "            cand = difflib.get_close_matches(tl, list(col_set.keys()), n=1, cutoff=0.6)\n",
    "            if cand:\n",
    "                replacements[tok] = col_set[cand[0]]\n",
    "\n",
    "    fixed = sql\n",
    "    for bad, good in replacements.items():\n",
    "        fixed = re.sub(rf\"\\b{re.escape(bad)}\\b\", good, fixed)\n",
    "\n",
    "    note = None\n",
    "    if replacements:\n",
    "        pairs = \", \".join([f\"`{b}`â†’`{g}`\" for b, g in replacements.items()])\n",
    "        note = f\"ðŸ©¹ Pre-fixed unknown identifiers: {pairs}\"\n",
    "    return fixed, note\n",
    "\n",
    "def try_explain_sql(conn, sql: str):\n",
    "    return pd.read_sql_query(\"EXPLAIN \" + sql, conn)\n",
    "\n",
    "def validate_and_fix_sql(sql: str, db_path: str, table: str, cols: list):\n",
    "    # 1) proactive pass: fix unknown identifiers before EXPLAIN\n",
    "    sql, pre_note = enforce_columns(sql, cols, table)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        try_explain_sql(conn, sql)  # if OK, weâ€™re done\n",
    "        return sql, pre_note\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        # 2) fallback: parse SQLite error for a specific bad column and try a single replacement\n",
    "        m = re.search(r\"no such column: ([\\w_]+)\", msg, re.I)\n",
    "        if m:\n",
    "            bad = m.group(1)\n",
    "            match = difflib.get_close_matches(bad.lower(), [c.lower() for c in cols], n=1, cutoff=0.6)\n",
    "            if match:\n",
    "                good = next(c for c in cols if c.lower() == match[0])\n",
    "                fixed = re.sub(rf\"\\b{re.escape(bad)}\\b\", good, sql, flags=re.I)\n",
    "                try:\n",
    "                    try_explain_sql(conn, fixed)\n",
    "                    note2 = f\"ðŸ©¹ Replaced `{bad}` â†’ `{good}`\"\n",
    "                    note = f\"{pre_note}\\n{note2}\" if pre_note else note2\n",
    "                    return fixed, note\n",
    "                except Exception:\n",
    "                    pass\n",
    "        # 3) give up with message\n",
    "        return None, f\"Validation failed: {msg}\"\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "def validate_and_fix_sql(sql: str, db_path: str, table: str, cols: list):\n",
    "    # 0) hard replacements first\n",
    "    sql0 = apply_strict_aliases(sql)\n",
    "\n",
    "    # 1) proactive pass: fix unknown identifiers before EXPLAIN\n",
    "    sql1, pre_note = enforce_columns(sql0, cols, table)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    try:\n",
    "        try_explain_sql(conn, sql1)  # if OK, done\n",
    "        return sql1, pre_note\n",
    "    except Exception as e:\n",
    "        msg = str(e)\n",
    "        # fallback: parse specific bad column & fuzzy replace\n",
    "        m = re.search(r\"no such column: ([\\w_]+)\", msg, re.I)\n",
    "        if m:\n",
    "            bad = m.group(1)\n",
    "            match = difflib.get_close_matches(bad.lower(), [c.lower() for c in cols], n=1, cutoff=0.6)\n",
    "            if match:\n",
    "                good = next(c for c in cols if c.lower() == match[0])\n",
    "                fixed = re.sub(rf\"\\b{re.escape(bad)}\\b\", good, sql1, flags=re.I)\n",
    "                try:\n",
    "                    try_explain_sql(conn, fixed)\n",
    "                    note2 = f\"ðŸ©¹ Replaced `{bad}` â†’ `{good}`\"\n",
    "                    note = f\"{pre_note}\\n{note2}\" if pre_note else note2\n",
    "                    return fixed, note\n",
    "                except Exception:\n",
    "                    pass\n",
    "        return None, f\"Validation failed: {msg}\"\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b625ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5) end-to-end guarded generation ---\n",
    "def generate_sql_guarded(question: str, db_path=\"../data/housing.db\", table=\"housing\", max_attempts=2):\n",
    "    prompt, cols = build_prompt(db_path, table, question)\n",
    "    sql = generate_sql(prompt)  # uses your working HF model function\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        fixed_sql, note = validate_and_fix_sql(sql, db_path, table, cols)\n",
    "        if fixed_sql:\n",
    "            if note:\n",
    "                print(note)\n",
    "            return fixed_sql\n",
    "        # If invalid, re-prompt the model with the error and allowed columns\n",
    "        error_msg = note or \"Invalid SQL.\"\n",
    "        reprompt = (\n",
    "            f\"{prompt}\\n\\nPrevious SQL:\\n{sql}\\n\\n\"\n",
    "            f\"Error: {error_msg}\\n\"\n",
    "            f\"Regenerate a valid SQL using ONLY these columns: {', '.join(cols)}.\\n\"\n",
    "            f\"Return just the SQL.\"\n",
    "        )\n",
    "        sql = generate_sql(reprompt)\n",
    "\n",
    "    return sql  # best effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "80c8429b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude', 'MedHouseVal']\n",
      "SELECT AVG(HouseAge) FROM housing WHERE MedInc > 5 AND Latitude > 35\n",
      "   addr    opcode  p1  p2  p3    p4  p5 comment\n",
      "0     0      Init   0  18   0  None   0    None\n",
      "1     1      Null   0   1   2  None   0    None\n",
      "2     2  OpenRead   0   2   0     7   0    None\n",
      "3     3    Rewind   0  14   0  None   0    None\n",
      "4     4    Column   0   0   3  None   0    None\n"
     ]
    }
   ],
   "source": [
    "# 1) See actual columns the validator will enforce:\n",
    "ddl, cols = get_schema_and_columns(\"../data/housing.db\", \"housing\")\n",
    "print(cols)  # must include HouseAge, MedInc, Latitude, etc.\n",
    "\n",
    "# 2) Generate with guard\n",
    "q = \"Average house age for blocks with MedInc > 5 and Latitude > 35\"\n",
    "sql = generate_sql_guarded(q)\n",
    "print(sql)\n",
    "\n",
    "# 3) Execute to confirm no errors:\n",
    "import pandas as pd, sqlite3\n",
    "with sqlite3.connect(\"../data/housing.db\") as conn:\n",
    "    print(pd.read_sql_query(\"EXPLAIN \" + sql, conn).head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "45ac9000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Final SQL: SELECT AVG(HouseAge) FROM housing WHERE MedInc > 5 AND Latitude > 35\n"
     ]
    }
   ],
   "source": [
    "question = \"Average house age for blocks with MedInc > 5 and Latitude > 35\"\n",
    "safe_sql = generate_sql_guarded(question)\n",
    "print(\"âœ… Final SQL:\", safe_sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e4b3f8",
   "metadata": {},
   "source": [
    "### Execute the suggested SQL query from the housing.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bb6900fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(26.02671755725191,)]\n"
     ]
    }
   ],
   "source": [
    "# execute the generated SQL query against the database\n",
    "def run_query(db_path, query):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    conn.close()\n",
    "    return results\n",
    "results = run_query(db_path, safe_sql)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49a8fb3",
   "metadata": {},
   "source": [
    "### Store query history table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "20a9f81d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Logged to query_history\n"
     ]
    }
   ],
   "source": [
    "import sqlite3, time\n",
    "with sqlite3.connect(\"../data/housing.db\") as conn:\n",
    "    conn.execute(\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS query_history(\n",
    "        ts INTEGER, question TEXT, sql TEXT\n",
    "      )\n",
    "    \"\"\")\n",
    "    conn.execute(\"INSERT INTO query_history VALUES(?,?,?)\",\n",
    "                 (int(time.time()), question, safe_sql))\n",
    "print(\"âœ… Logged to query_history\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b9b6a",
   "metadata": {},
   "source": [
    "### Save outputs\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "a217a56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Query saved to ..\\data\\generated_queries.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path(\"../data/generated_queries.csv\")\n",
    "\n",
    "# Example: log prompt + SQL\n",
    "log = pd.DataFrame([{\n",
    "    \"input_prompt\": input_prompt,\n",
    "    \"generated_sql\": safe_sql\n",
    "}])\n",
    "\n",
    "if results_file.exists():\n",
    "    log.to_csv(results_file, mode=\"a\", header=False, index=False)\n",
    "else:\n",
    "    log.to_csv(results_file, index=False)\n",
    "\n",
    "print(\"âœ… Query saved to\", results_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
